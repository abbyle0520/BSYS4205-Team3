{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "fashion_mnist_keras.ipynb_Team 3B (Abigail, Aikemu, Cindy, Kooshan).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbyle0520/BSYS4205-Team3/blob/main/fashion_mnist_keras_ipynb_Team_3B_(Abigail%2C_Aikemu%2C_Cindy%2C_Kooshan).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoTrBxfwHc7m"
      },
      "source": [
        "# BSYS-4205"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McScw-pnHc7o"
      },
      "source": [
        "# Team Project: AI/ML Hands-on Application: Analyzing Image Recognition Using Fashion_mnst Dataset\n",
        "Team 3B:\n",
        "\n",
        "Nguyen Phuong Anh (Abigail) Le (A01173433)\n",
        "\n",
        "Minh Chau (Cindy) Ly (A01061833)\n",
        "\n",
        "Aikemu Yilizhati (A........)\n",
        "\n",
        "Hamidreza Shahidizandi (Kooshan) (A01177947)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xh8iwZBHc7p"
      },
      "source": [
        "#Contents \n",
        "\n",
        "1. Foreword\n",
        "\n",
        "2. Use Case Objective \n",
        "\n",
        "3. Dataset description and sample data display\n",
        "\n",
        "4. Algorithms used in the case study: How It Works and Parameters\n",
        "\n",
        "5. Python Code: Jupyter Notebook Experiment\n",
        "\n",
        "6. Scenarios\n",
        "\n",
        "7. Result Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZYDRPpBHc7p"
      },
      "source": [
        "# Foreword\n",
        "In this project documentation, we will be presenting how Image Recognition technique was applied onto the Fashion Dataset by Margaret Maynard-Reid (4/24/2018) and exploring how the algorithms work.\n",
        "\n",
        "We will also tweak and improve this dataset by testing out variables in order to achieve new - and possibly more effective result.\n",
        "\n",
        "Our GitHub repo link can be found here: https://github.com/abbyle0520/BSYS4205-Team3\n",
        "\n",
        "The original JupyterNotebook where the dataset research was conducted by Margaret Maynard-Reid can be found here: https://colab.research.google.com/github/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIpc-r8NHc7p"
      },
      "source": [
        "# Use Case objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVl9j7pCVDNc"
      },
      "source": [
        "**Convolutional Neural Network**\r\n",
        "\r\n",
        "In order to process and work with the mentioned dataset, we will be using the Convolutional Neural Network class model. \r\n",
        "\r\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/usecaseobjective1_Abi.png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "From this traditional model, the Convolutional Neural Network class model has been built and applied to many aspects of Deep Learning, such as: image and video recognition, image classification, medical image analysis, …\r\n",
        "\r\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/usecaseobjective2_Abi.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvHHG13XHc7q"
      },
      "source": [
        "\n",
        "From what we have discussed, we have utilized CNN model to use the fashion item image as the input, and the hidden layers are the shapes or characteristics of mentioned item. Precisely, the features that differentiate a bag from a dress. \n",
        "A more detailed look into this classification network model can be seen below:\n",
        "\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/usecaseobjective3_Abi.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnJbSUPHc7q"
      },
      "source": [
        "# Dataset Description and Sample Data Display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdTdq41Hc7r"
      },
      "source": [
        "#General Description \n",
        "\n",
        "Fashion-MNIST is a dataset comprises of Zalando’s article images (a training set of 60,000 examples and a test set of 10,000 examples.) This dataset was created with the intention to be used as a replacement for the original MNIST dataset which was used for benchmarking machine learning algorithms.\n",
        "\n",
        "#Dataset Description \n",
        "\n",
        "Fashion-MNIST dataset has the same size as MNIST but the images are labelled by tags. Every image is a NumPy 28x28 array, with pixel values between 0 and 255. The labels are an array of integers, from 0 to 9. These labels correspond to the type of clothing the image represents.\n",
        "\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/DatasetLabel_Abi.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrll4Z79Hc7r"
      },
      "source": [
        "The dataset can be found on its homepage: https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYo6jTrGHc7s"
      },
      "source": [
        "First, we can take a look at the shapes of images and labels by using Tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjlhsADRHc7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbf5372-0ce8-41e8-fe2e-50a2ec62c408"
      },
      "source": [
        "# Install Tensorflow and Import needed libraries\n",
        "\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the fashion-mnist data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.27.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (53.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6niNb2eaHc7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7423a075-1b0c-41bd-b5b8-55f1144036d7"
      },
      "source": [
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "print(x_train.shape[0], 'training data sets')\n",
        "print(x_test.shape[0], 'test data sets')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "60000 training data sets\n",
            "10000 test data sets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Thn85MHc7t"
      },
      "source": [
        "From the output we learned that this dataset has **60,000** training data, the image size for each image is **28x28**, and there are also **10,000** test labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0OlzUNdHc7t"
      },
      "source": [
        "# Sample Data Display \n",
        "Our dataset comprises of clothing items in a low resolution (28 x 28 pixels), as seen here:\n",
        "\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/DatasetSample_Abi.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6CYH1LHc7u"
      },
      "source": [
        "For furthermore exploration of the data, we will be covering later in the *“Explore the Dataset”* section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FINskN2OP6r"
      },
      "source": [
        "#Algorithms used in the case study. how do they work? what are their parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgWZu-pEOhIs"
      },
      "source": [
        "##Parameters\r\n",
        "\r\n",
        "This function traverses the data set, performs output computation from the input, measures the error using the loss function, optimizes the model using the optimizer, and then repeats the process. Going through the data one at a time is called an epoch completion.\r\n",
        "\r\n",
        "The model.fit method includes:\r\n",
        "\r\n",
        "•\tTraining and test dataset,\r\n",
        "\r\n",
        "•\tbatch_size: the number of samples that Mini-batch GD uses for each weight updated.\r\n",
        "\r\n",
        "•\tEpoch: the number of times through all the number of samples in the training dataset.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZQyd3eAO3F9"
      },
      "source": [
        "#Algorithms\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3vTZVGTS_8A"
      },
      "source": [
        "##Softmax regression\r\n",
        "\r\n",
        "Each layer will perform 2 steps: calculating the linear sum of the nodes in the previous layer and performing the activation function. Since each image will belong to a class from 0 to 9, there will be 10 classes in total. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzXxrfZ5PPOB"
      },
      "source": [
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/softmax_Cindy.png?raw=true)\r\n",
        "\r\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/softmax2_Cindy.png?raw=true)\r\n",
        "\r\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/softmax3_Cindy.png?raw=true)\r\n",
        "\r\n",
        "For classification problems, if there are 2 classes, the activation function at the output layer is the sigmoid function, and more than 2 classes, the activation function in the ouput layer is the softmax function.\r\n",
        "\r\n",
        "=> Output layer has 10 nodes and activation is softmax function.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhlV3F1BQxjE"
      },
      "source": [
        "# Loss function\r\n",
        "\r\n",
        "To define the loss function, we first use one-hot encoding which converts the label of the image from a numeric value to a vector of the same size as the model output.\r\n",
        "\r\n",
        "![alt text](https://github.com/abbyle0520/BSYS4205-Team3/blob/main/lossfunction_Cindy.png?raw=true)\r\n",
        "\r\n",
        "\r\n",
        "For example: the label of data is the number i is a vector and size 10 * 1 with vi+1 =1 and other values equal to 0. \r\n",
        "\r\n",
        "Compared to the above convention of percentages, one-hot encoding means that this image is Sneaker. Now we have the real value y (label) in the form of one-hot encoding the predicted value on the output layer after the softmax function of the same size 10 * 1. \r\n",
        "\r\n",
        "The loss function as defined above in Keras is called \"categorical_crossentropy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52P95RMAdHti"
      },
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeovFar3dJuh"
      },
      "source": [
        "#Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIFMGqTiRD1Z"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWvYdVRKS0MD"
      },
      "source": [
        "Here we’re using Keras which is a high level API to build and train models. We’re using Keras for three main reasons:\r\n",
        "\r\n",
        "1.\tUser Friendly\r\n",
        "2.\tModular\r\n",
        "3.\tEasy to Extend.\r\n",
        "\r\n",
        "Building the neural network requires configuring the layers of the model and then compiling the model.\r\n",
        "\r\n",
        "Layers are the basic block of neural network and they extract representation from the data fed into them and represent them in a more meaningful way.\r\n",
        "As you can see here we have keras.sequential and we’re using **Conv2D, MaxPooling2D, Dropout, Flatten, and Dense** layers to build our model.\r\n",
        "\r\n",
        "**Keras Conv2D** is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\r\n",
        "\r\n",
        "**Kernel**: In image processing kernel is a convolution matrix or masks which can be used for blurring, sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.\r\n",
        "\r\n",
        "**Filter**: Number of Filters is a Mandatory Conv2D parameter and its an integer that convolutional layers will learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziPnzxD0d4za"
      },
      "source": [
        "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. \r\n",
        "\r\n",
        "In line 12 the flatten layer changes the 2d array of 28 by 28 to a 1d array.\r\n",
        "\r\n",
        "In line 13 and 14, were using Dense layer and these two layers are densely connected in the first layer we have 256 nodes or neurons while the second one has 10 nodes with soft max layer and this returns an array of 10 probability scores that sum to 1.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KWWZlCOUYzx",
        "outputId": "7f775759-1b22-457e-fb08-a7fdea3cc769"
      },
      "source": [
        "model = tf.keras.Sequential()\r\n",
        "\r\n",
        "# Must define the input shape in the first layer of the neural network\r\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \r\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\r\n",
        "model.add(tf.keras.layers.Dropout(0.3))\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\r\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\r\n",
        "model.add(tf.keras.layers.Dropout(0.3))\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Flatten())\r\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\r\n",
        "model.add(tf.keras.layers.Dropout(0.5))\r\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "# Take a look at the model summary\r\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        8224      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               401664    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 412,778\n",
            "Trainable params: 412,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmKKflW5d-AY"
      },
      "source": [
        "The next step before training the model is adding a few more setting which can be done during the model compile step. \r\n",
        "\r\n",
        "For example, here we have loss function which measures how accurate the model is during the training and we want to minimize the function to see the model is in the right direction. \r\n",
        "\r\n",
        "Another parameter is optimizer which is how the model is updated based on the data and finally, we have the metric which is used to monitor the training and testing steps which In our example we’re using accuracy which is the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQcxyI4KeAX3"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95itlTC2dLyy"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3ouYamfePye"
      },
      "source": [
        "Here we’re using ModelCheckpoint API to save the model after every epoch. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.\r\n",
        "\r\n",
        "In the Model.fit(), we have:\r\n",
        "-\t x_train as our input data and y_train as our target data\r\n",
        "-\tBatch_size : Number of samples per gradient update. \r\n",
        "-\tEpochs: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\r\n",
        "-\tValidation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "npzfud4debs7",
        "outputId": "db8aa88a-00bf-443b-c8b1-f9ef709e49c9"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\r\n",
        "model.fit(x_train,y_train,\r\n",
        "          batch_size=64,\r\n",
        "          epochs=10,\r\n",
        "          validation_data=(x_valid,y_valid),\r\n",
        "          callbacks=[checkpointer])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-00aaf9d052be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m           callbacks=[checkpointer])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_valid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SoYxmXgdTXv"
      },
      "source": [
        "# Feed the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcMQYxHYfNyK"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KobA1t81dV7J"
      },
      "source": [
        "# Python Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph9OuJkideg1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEWAJHgdatH"
      },
      "source": [
        "#Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2PqORMQfQUa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6PRodxqdc8G"
      },
      "source": [
        "#Result Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btFdsZrUfRo9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvCj_6qGUZhr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}